{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9366a6b0-c26b-4f42-8083-8f41af00790b",
   "metadata": {},
   "source": [
    "### Object detection AP ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a014abab-92e8-4939-a3e1-7b63ba7a8f4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Project module version: 0.0.1.post1.dev31+g14d1a25.d20250315\n",
      "PyTorch version:        2.6.0a0+ecf3bae40a.nv25.01\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ast\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import patches\n",
    "\n",
    "# PyTorch and HuggingFace Transformers\n",
    "import torch\n",
    "from torchvision import ops\n",
    "from transformers import RTDetrV2ForObjectDetection, RTDetrImageProcessor\n",
    "\n",
    "logger = logging.getLogger(name=__name__)\n",
    "\n",
    "# Appearance of the Notebook\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "np.set_printoptions(linewidth=110)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# Import this module with autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import detection as dt\n",
    "from detection.fileutils import FileOP\n",
    "from detection.imageproc import ImageData, xyxy2xywh, xywh2xyxy, clipxywh\n",
    "from detection.imageproc import determine_bbox_format\n",
    "from detection.performance import get_iou\n",
    "\n",
    "print(f'Project module version: {dt.__version__}')\n",
    "print(f'PyTorch version:        {torch.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be660273-a1bb-4afa-a12d-1fc1e4c4f5d2",
   "metadata": {},
   "source": [
    "### Helper functions ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "90f2e4e2-3c72-4fe0-bd67-36276b7f96c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to show an image with bounding boxes\n",
    "def show_image_with_boxes(image, box_list, label_list=None, alpha=0.7, colors=None, ax=None):\n",
    "    if colors is None:\n",
    "        color = plt.cm.rainbow(np.linspace(0, 1, len(box_list)))\n",
    "        color_list = [color[c] for c in range(len(color))]\n",
    "    else:\n",
    "        assert len(colors)==len(box_list), f'Number of colors does not match number bounding boxes.'\n",
    "        color_list = colors.copy()\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(4, 4))\n",
    "    ax.imshow(image)\n",
    "    for b, box in enumerate(box_list):\n",
    "        boxcolor = color_list[b]\n",
    "        anchor = (box[0], box[1])\n",
    "        width = box[2]\n",
    "        height = box[3]\n",
    "        #anchor, width, height = rectangle(box)\n",
    "        rect = patches.Rectangle(xy=anchor, \n",
    "                                 width=width, \n",
    "                                 height=height, \n",
    "                                 linewidth=2.5, \n",
    "                                 edgecolor=boxcolor, \n",
    "                                 facecolor='none', \n",
    "                                 alpha=alpha)\n",
    "        ax.add_patch(rect)\n",
    "        if label_list is not None:\n",
    "            ax.text(x=anchor[0]+width/2, y=anchor[1]-3, s=label_list[b], color=boxcolor)\n",
    "    ax.set(xticks=[], yticks=[])\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "aae406d4-ed2c-43d8-aaca-c55cb8927e7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>quadrant</th>\n",
       "      <th>pos</th>\n",
       "      <th>bbox</th>\n",
       "      <th>segmentation</th>\n",
       "      <th>fdi</th>\n",
       "      <th>ada</th>\n",
       "      <th>dset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_0_1.png</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[666, 102, 103, 376]</td>\n",
       "      <td>[[757, 478, 769, 102, 678, 113, 666, 469]]</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_0_1.png</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[593, 107, 85, 377]</td>\n",
       "      <td>[[666, 484, 678, 110, 607, 107, 604, 299, 619,...</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       file_name  quadrant pos                  bbox                                       segmentation  fdi  ada   dset\n",
       "0  train_0_1.png         1   1  [666, 102, 103, 376]         [[757, 478, 769, 102, 678, 113, 666, 469]]   11    8  train\n",
       "1  train_0_1.png         1   2   [593, 107, 85, 377]  [[666, 484, 678, 110, 607, 107, 604, 299, 619,...   12    7  train"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Directories and files\n",
    "data_dir = os.path.join(os.environ.get('HOME'), 'data', 'dentex_detection')\n",
    "image_dir = os.path.join(data_dir, 'quadrants')\n",
    "\n",
    "# Model directory\n",
    "model_name = 'rtdetr_250311_06'\n",
    "model_dir = os.path.join(data_dir, 'models', model_name)\n",
    "\n",
    "# Log file\n",
    "log_file_name = 'train_log_250311.log'\n",
    "log_file = os.path.join(model_dir, log_file_name)\n",
    "\n",
    "# Output for the images with the predictions\n",
    "output_dir = os.path.join(model_dir, 'testresults')\n",
    "Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "dataset_df_file_name = 'train_split_250224.parquet'\n",
    "dataset_df_file = os.path.join(image_dir, dataset_df_file_name)\n",
    "df = pd.read_parquet(dataset_df_file)\n",
    "display(df.head(2))\n",
    "\n",
    "# Label to ADA postion conversion\n",
    "ada_list = sorted(list(df['ada'].unique()))\n",
    "id2label = dict(zip(range(len(ada_list)), ada_list))\n",
    "label2id = {v: k for k, v in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "78a0d86f-91be-4090-88b0-2beb44c9f3da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250311-21:25-__main__-INFO-{\"output_dir\": \"/app/data/dentex_detection/model/rtdetr_250311_06\", \"num_train_epochs\": 500, \"max_grad_norm\": 0.1, \"learning_rate\": 1e-05, \"warmup_steps\": 300, \"per_device_train_batch_size\": 48, \"dataloader_num_workers\": 8, \"metric_for_best_model\": \"eval_map\", \"greater_is_better\": true, \"load_best_model_at_end\": true, \"eval_strategy\": \"epoch\", \"save_strategy\": \"epoch\", \"save_total_limit\": 5, \"remove_unused_columns\": false, \"eval_do_concat_batches\": false}\n",
      "250311-21:25-__main__-INFO-{\"model_version\": 6, \"model_name\": \"rtdetr_250311_06\", \"hf_checkpoint\": \"PekingU/rtdetr_v2_r101vd\", \"use_transform\": \"transform_1\"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the log file\n",
    "with open(log_file, mode='r') as fl:\n",
    "    log = fl.read()\n",
    "print(log)\n",
    "\n",
    "# Load the model from checkpoint\n",
    "hf_model_name = 'PekingU/rtdetr_v2_r101vd'\n",
    "processor = RTDetrImageProcessor.from_pretrained(hf_model_name)\n",
    "checkpoint_name = 'checkpoint-10140'\n",
    "checkpoint_file = os.path.join(model_dir, checkpoint_name)\n",
    "model = RTDetrV2ForObjectDetection.from_pretrained(checkpoint_file).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a9445e-fa10-4692-bcf5-94a5160f5cfa",
   "metadata": {},
   "source": [
    "### Attempt to calculate AP for the model ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406b94cd-2b30-405b-a163-be6495c3346b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

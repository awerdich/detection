{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10856ab4-1837-475e-a3d4-cc0458fcc570",
   "metadata": {},
   "source": [
    "### Build the training script ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5915e27b-9f4c-4cde-b8eb-482878d32cae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Project module version: 0.0.1.post1.dev26+g0781d45\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import glob\n",
    "import json\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import patches\n",
    "\n",
    "# PyTorch framework\n",
    "import torch\n",
    "\n",
    "# Hugging Face Library\n",
    "from transformers import RTDetrV2ForObjectDetection, RTDetrImageProcessor\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# Appearance of the Notebook\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "np.set_printoptions(linewidth=110)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# Import this module with autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import detection as dt\n",
    "from detection.detrdataset import get_gpu_info, DetectionDatasetFromDF\n",
    "from detection.detransform import DetrTransform\n",
    "from detection.imageproc import clipxywh, ImageData\n",
    "from detection.mapeval import MAPEvaluator\n",
    "\n",
    "print(f'Project module version: {dt.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4d37f6ef-6c71-4678-bb16-a33a927584c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Number of GPUs found:  1\n",
      "Current device ID: 0\n",
      "GPU device name:   NVIDIA GeForce RTX 3060 Laptop GPU\n",
      "PyTorch version:   2.6.0a0+ecf3bae40a.nv25.01\n",
      "CUDA version:      12.8\n",
      "CUDNN version:     90700\n",
      "Date: 250224\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device, device_str = get_gpu_info()\n",
    "\n",
    "# Save the date in a string\n",
    "date_str = datetime.date.today().strftime('%y%m%d')\n",
    "print(f'Date: {date_str}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dd25a9-8f1b-40af-a003-e83cb79e434a",
   "metadata": {},
   "source": [
    "### Locations for the data ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ebd030c2-5399-4e6a-a065-cbe0296634c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model: rtdetr_250224_01\n",
      "Found 2531 images.\n"
     ]
    }
   ],
   "source": [
    "# Data directory\n",
    "data_root = os.path.join(os.environ.get('HOME'), 'data')\n",
    "data_dir = os.path.join(data_root, 'dentex_detection')\n",
    "\n",
    "# Save model checkpoints\n",
    "model_dir = os.path.join(data_dir, 'model')\n",
    "Path(model_dir).mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Model name\n",
    "model_version = 1\n",
    "model_name = f'rtdetr_{date_str}_{str(model_version).zfill(2)}'\n",
    "print(f'Training model: {model_name}')\n",
    "\n",
    "# Log files\n",
    "log_dir = os.path.join(model_dir, 'log')\n",
    "Path(log_dir).mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Image directory and annotation data\n",
    "image_dir = os.path.join(data_dir, 'quadrants')\n",
    "annotation_file_name = 'train_split_250224.parquet'\n",
    "annotation_file = os.path.join(image_dir, annotation_file_name)\n",
    "\n",
    "# Check the images on disk\n",
    "file_list = glob.glob(os.path.join(image_dir, '*.png'))\n",
    "expected_n_images = 2531\n",
    "if not len(file_list) == expected_n_images:\n",
    "    print(f'WARNING: expected number of images ({expected_n_images}) does not match the number of images on disk.')\n",
    "    print(f'Delete files and start over.')\n",
    "else:\n",
    "    print(f'Found {len(file_list)} images.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b17a5a-cd94-4094-9bfc-bc57f7607fbe",
   "metadata": {},
   "source": [
    "### Load the annotations ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "51165adc-eedf-46cc-a402-ccce020ae6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(annotation_file)\n",
    "# Create the label column (the tooth position, but starting from 0)\n",
    "label_name_list = sorted(list(df['ada'].unique()))\n",
    "id2label = dict(zip(range(len(label_name_list)), label_name_list))\n",
    "id2label = {int(label_id): str(name) for label_id, name in id2label.items()}\n",
    "label2id = {str(name): int(label_id) for label_id, name in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b87e1806-00be-4e89-95c3-fa97a1c0b8bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>quadrant</th>\n",
       "      <th>pos</th>\n",
       "      <th>bbox</th>\n",
       "      <th>segmentation</th>\n",
       "      <th>fdi</th>\n",
       "      <th>ada</th>\n",
       "      <th>dset</th>\n",
       "      <th>file</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_0_1.png</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[666, 102, 103, 376]</td>\n",
       "      <td>[[757, 478, 769, 102, 678, 113, 666, 469]]</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>train</td>\n",
       "      <td>/app/data/dentex_detection/quadrants/train_0_1...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_0_1.png</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[593, 107, 85, 377]</td>\n",
       "      <td>[[666, 484, 678, 110, 607, 107, 604, 299, 619,...</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>train</td>\n",
       "      <td>/app/data/dentex_detection/quadrants/train_0_1...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_0_1.png</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>[531, 69, 85, 368]</td>\n",
       "      <td>[[587, 437, 616, 357, 607, 72, 534, 69, 531, 4...</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>train</td>\n",
       "      <td>/app/data/dentex_detection/quadrants/train_0_1...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_0_1.png</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>[457, 31, 115, 403]</td>\n",
       "      <td>[[522, 434, 572, 378, 543, 31, 463, 40, 457, 3...</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>train</td>\n",
       "      <td>/app/data/dentex_detection/quadrants/train_0_1...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_0_1.png</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>[369, 10, 100, 406]</td>\n",
       "      <td>[[437, 416, 469, 378, 466, 10, 381, 31, 378, 2...</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>train</td>\n",
       "      <td>/app/data/dentex_detection/quadrants/train_0_1...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       file_name  quadrant pos                  bbox                                       segmentation  fdi  ada   dset                                               file  label\n",
       "0  train_0_1.png         1   1  [666, 102, 103, 376]         [[757, 478, 769, 102, 678, 113, 666, 469]]   11    8  train  /app/data/dentex_detection/quadrants/train_0_1...      7\n",
       "1  train_0_1.png         1   2   [593, 107, 85, 377]  [[666, 484, 678, 110, 607, 107, 604, 299, 619,...   12    7  train  /app/data/dentex_detection/quadrants/train_0_1...      6\n",
       "2  train_0_1.png         1   3    [531, 69, 85, 368]  [[587, 437, 616, 357, 607, 72, 534, 69, 531, 4...   13    6  train  /app/data/dentex_detection/quadrants/train_0_1...      5\n",
       "3  train_0_1.png         1   4   [457, 31, 115, 403]  [[522, 434, 572, 378, 543, 31, 463, 40, 457, 3...   14    5  train  /app/data/dentex_detection/quadrants/train_0_1...      4\n",
       "4  train_0_1.png         1   5   [369, 10, 100, 406]  [[437, 416, 469, 378, 466, 10, 381, 31, 378, 2...   15    4  train  /app/data/dentex_detection/quadrants/train_0_1...      3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]\n"
     ]
    }
   ],
   "source": [
    "# Add columns with the file paths and the labels\n",
    "df = df.assign(file=df['file_name'].apply(lambda f: os.path.join(image_dir, f)),\n",
    "               label=df['ada'].apply(lambda name: label2id.get(str(name))))\n",
    "display(df.head())\n",
    "print(sorted(list(df['label'].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9241c443-2faa-4259-b451-806c870c9c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RTDetrV2ForObjectDetection were not initialized from the model checkpoint at PekingU/rtdetr_v2_r101vd and are newly initialized because the shapes did not match:\n",
      "- model.decoder.class_embed.0.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([32]) in the model instantiated\n",
      "- model.decoder.class_embed.0.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([32, 256]) in the model instantiated\n",
      "- model.decoder.class_embed.1.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([32]) in the model instantiated\n",
      "- model.decoder.class_embed.1.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([32, 256]) in the model instantiated\n",
      "- model.decoder.class_embed.2.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([32]) in the model instantiated\n",
      "- model.decoder.class_embed.2.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([32, 256]) in the model instantiated\n",
      "- model.decoder.class_embed.3.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([32]) in the model instantiated\n",
      "- model.decoder.class_embed.3.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([32, 256]) in the model instantiated\n",
      "- model.decoder.class_embed.4.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([32]) in the model instantiated\n",
      "- model.decoder.class_embed.4.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([32, 256]) in the model instantiated\n",
      "- model.decoder.class_embed.5.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([32]) in the model instantiated\n",
      "- model.decoder.class_embed.5.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([32, 256]) in the model instantiated\n",
      "- model.denoising_class_embed.weight: found shape torch.Size([81, 256]) in the checkpoint and torch.Size([33, 256]) in the model instantiated\n",
      "- model.enc_score_head.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([32]) in the model instantiated\n",
      "- model.enc_score_head.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([32, 256]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Augmentations\n",
    "use_transform = 'transform_1'\n",
    "\n",
    "# Hugging face model checkpoint\n",
    "hf_checkpoint = 'PekingU/rtdetr_v2_r101vd'\n",
    "image_processor = RTDetrImageProcessor.from_pretrained(hf_checkpoint)\n",
    "model = RTDetrV2ForObjectDetection.from_pretrained(hf_checkpoint,\n",
    "                                                   id2label=id2label,\n",
    "                                                   label2id=label2id,\n",
    "                                                   anchor_image_size=None,\n",
    "                                                   ignore_mismatched_sizes=True)\n",
    "\n",
    "# Custom collate_fn to batch the images\n",
    "def collate_fn(batch):\n",
    "    data = {}\n",
    "    data[\"pixel_values\"] = torch.stack([x[\"pixel_values\"] for x in batch])\n",
    "    data[\"labels\"] = [x[\"labels\"] for x in batch]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1529f8d5-4ca0-4b53-be97-efab480629f9",
   "metadata": {},
   "source": [
    "### Set up logging ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "351c35ac-8038-4be3-b95b-3432dc8e88ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file_name = f'train_log_{date_str}.log'\n",
    "log_file = os.path.join(log_dir, log_file_name)\n",
    "dtfmt = '%y%m%d-%H:%M'\n",
    "logfmt = '%(asctime)s-%(name)s-%(levelname)s-%(message)s'\n",
    "logging.basicConfig(filename=log_file,\n",
    "                    filemode='w',\n",
    "                    level=logging.INFO,\n",
    "                    format=logfmt,\n",
    "                    datefmt=dtfmt)\n",
    "logger = logging.getLogger(name=__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d31742-7d37-4ee3-872d-6e8c138e7608",
   "metadata": {},
   "source": [
    "### Training arguments ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3d4a0350-6bfd-4116-bdfa-dc73c57a11d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args_dict = {'output_dir': os.path.join(model_dir, model_name), \n",
    "                      'num_train_epochs': 20, \n",
    "                      'max_grad_norm': 0.1, \n",
    "                      'learning_rate': 5e-5, \n",
    "                      'warmup_steps': 300,\n",
    "                      'per_device_train_batch_size': 4, \n",
    "                      'dataloader_num_workers': 2, \n",
    "                      'metric_for_best_model': 'eval_map',\n",
    "                      'greater_is_better': True, \n",
    "                      'load_best_model_at_end': True,\n",
    "                      'eval_strategy': 'epoch', \n",
    "                      'save_strategy': 'epoch', \n",
    "                      'save_total_limit': 2, \n",
    "                      'remove_unused_columns': False,\n",
    "                      'eval_do_concat_batches': False}\n",
    "\n",
    "logger.info(json.dumps(training_args_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a239195-eaf3-4708-93be-0013992041df",
   "metadata": {},
   "source": [
    "### Create the data sets ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "07513356-c17f-474d-8af2-abb89657dbff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test', 'train', 'val']\n",
      "Number of images in TEST: 32\n",
      "Number of images in TRAIN: 2479\n",
      "Number of images in VAL: 20\n"
     ]
    }
   ],
   "source": [
    "# Create the image transformations for the data sets\n",
    "train_transform = DetrTransform(use_transform).train_transform()\n",
    "val_transform = DetrTransform(use_transform).val_transform()\n",
    "\n",
    "# Create the data sets\n",
    "dset_list = sorted(list(df['dset'].unique()))\n",
    "print(dset_list)\n",
    "dataset_dict = {}\n",
    "for dset in dset_list:\n",
    "    df_dset = df.loc[df['dset'] == dset]\n",
    "    if dset == 'train':\n",
    "        transform = train_transform\n",
    "    else:\n",
    "        transform = val_transform\n",
    "    dataset = DetectionDatasetFromDF(data=df_dset, \n",
    "                                     processor=image_processor, \n",
    "                                     file_col='file', \n",
    "                                     label_col='label', \n",
    "                                     bbox_col='bbox', \n",
    "                                     transform=transform, \n",
    "                                     bbox_format='xywh', \n",
    "                                     validate=True)\n",
    "    dataset_dict.update({dset: dataset})\n",
    "    print(f'Number of images in {dset.upper()}: {len(dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ef4b2caa-802e-4600-b685-7601cb1a841b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the evaluation metrics\n",
    "eval_compute_metrics_fn = MAPEvaluator(image_processor=image_processor, threshold=0.01, id2label=id2label)\n",
    "training_args = TrainingArguments(**training_args_dict)\n",
    "\n",
    "# Set up the training\n",
    "trainer = Trainer(model=model,\n",
    "                  args=training_args,\n",
    "                  train_dataset=dataset_dict.get('train'),\n",
    "                  eval_dataset=dataset_dict.get('val'),\n",
    "                  processing_class=image_processor,\n",
    "                  data_collator=collate_fn,\n",
    "                  compute_metrics=eval_compute_metrics_fn)\n",
    "\n",
    "# Run the training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b24e8a9e-b84c-4413-9d11-49bf3b34b71e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1250' max='12400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1250/12400 13:04 < 1:56:48, 1.59 it/s, Epoch 2.01/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Map</th>\n",
       "      <th>Map 50</th>\n",
       "      <th>Map 75</th>\n",
       "      <th>Map Small</th>\n",
       "      <th>Map Medium</th>\n",
       "      <th>Map Large</th>\n",
       "      <th>Mar 1</th>\n",
       "      <th>Mar 10</th>\n",
       "      <th>Mar 100</th>\n",
       "      <th>Mar Small</th>\n",
       "      <th>Mar Medium</th>\n",
       "      <th>Mar Large</th>\n",
       "      <th>Map 1</th>\n",
       "      <th>Mar 100 1</th>\n",
       "      <th>Map 2</th>\n",
       "      <th>Mar 100 2</th>\n",
       "      <th>Map 3</th>\n",
       "      <th>Mar 100 3</th>\n",
       "      <th>Map 4</th>\n",
       "      <th>Mar 100 4</th>\n",
       "      <th>Map 5</th>\n",
       "      <th>Mar 100 5</th>\n",
       "      <th>Map 6</th>\n",
       "      <th>Mar 100 6</th>\n",
       "      <th>Map 7</th>\n",
       "      <th>Mar 100 7</th>\n",
       "      <th>Map 8</th>\n",
       "      <th>Mar 100 8</th>\n",
       "      <th>Map 9</th>\n",
       "      <th>Mar 100 9</th>\n",
       "      <th>Map 10</th>\n",
       "      <th>Mar 100 10</th>\n",
       "      <th>Map 11</th>\n",
       "      <th>Mar 100 11</th>\n",
       "      <th>Map 12</th>\n",
       "      <th>Mar 100 12</th>\n",
       "      <th>Map 13</th>\n",
       "      <th>Mar 100 13</th>\n",
       "      <th>Map 14</th>\n",
       "      <th>Mar 100 14</th>\n",
       "      <th>Map 15</th>\n",
       "      <th>Mar 100 15</th>\n",
       "      <th>Map 16</th>\n",
       "      <th>Mar 100 16</th>\n",
       "      <th>Map 17</th>\n",
       "      <th>Mar 100 17</th>\n",
       "      <th>Map 18</th>\n",
       "      <th>Mar 100 18</th>\n",
       "      <th>Map 19</th>\n",
       "      <th>Mar 100 19</th>\n",
       "      <th>Map 20</th>\n",
       "      <th>Mar 100 20</th>\n",
       "      <th>Map 21</th>\n",
       "      <th>Mar 100 21</th>\n",
       "      <th>Map 22</th>\n",
       "      <th>Mar 100 22</th>\n",
       "      <th>Map 23</th>\n",
       "      <th>Mar 100 23</th>\n",
       "      <th>Map 24</th>\n",
       "      <th>Mar 100 24</th>\n",
       "      <th>Map 25</th>\n",
       "      <th>Mar 100 25</th>\n",
       "      <th>Map 26</th>\n",
       "      <th>Mar 100 26</th>\n",
       "      <th>Map 27</th>\n",
       "      <th>Mar 100 27</th>\n",
       "      <th>Map 28</th>\n",
       "      <th>Mar 100 28</th>\n",
       "      <th>Map 29</th>\n",
       "      <th>Mar 100 29</th>\n",
       "      <th>Map 30</th>\n",
       "      <th>Mar 100 30</th>\n",
       "      <th>Map 31</th>\n",
       "      <th>Mar 100 31</th>\n",
       "      <th>Map 32</th>\n",
       "      <th>Mar 100 32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>38.122400</td>\n",
       "      <td>14.336624</td>\n",
       "      <td>0.386100</td>\n",
       "      <td>0.668700</td>\n",
       "      <td>0.410500</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.389900</td>\n",
       "      <td>0.505000</td>\n",
       "      <td>0.687800</td>\n",
       "      <td>0.726100</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.732900</td>\n",
       "      <td>0.303300</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.484800</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.236400</td>\n",
       "      <td>0.833300</td>\n",
       "      <td>0.453300</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.063400</td>\n",
       "      <td>0.675000</td>\n",
       "      <td>0.478100</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.394800</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>0.653000</td>\n",
       "      <td>0.840000</td>\n",
       "      <td>0.449400</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.531800</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>0.477500</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.202100</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>0.367300</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.372600</td>\n",
       "      <td>0.833300</td>\n",
       "      <td>0.519200</td>\n",
       "      <td>0.780000</td>\n",
       "      <td>0.250200</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.733300</td>\n",
       "      <td>0.110400</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.327300</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.049700</td>\n",
       "      <td>0.766700</td>\n",
       "      <td>0.335100</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.505000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.418200</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>0.223300</td>\n",
       "      <td>0.740000</td>\n",
       "      <td>0.214900</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>0.450400</td>\n",
       "      <td>0.540000</td>\n",
       "      <td>0.355400</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>0.488400</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.530600</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.662400</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.542800</td>\n",
       "      <td>0.740000</td>\n",
       "      <td>0.653200</td>\n",
       "      <td>0.775000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>23.280000</td>\n",
       "      <td>13.048721</td>\n",
       "      <td>0.495000</td>\n",
       "      <td>0.780000</td>\n",
       "      <td>0.523300</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500600</td>\n",
       "      <td>0.581100</td>\n",
       "      <td>0.689800</td>\n",
       "      <td>0.722400</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.727700</td>\n",
       "      <td>0.322200</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.402300</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.600300</td>\n",
       "      <td>0.866700</td>\n",
       "      <td>0.215000</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.702900</td>\n",
       "      <td>0.725000</td>\n",
       "      <td>0.632000</td>\n",
       "      <td>0.740000</td>\n",
       "      <td>0.462600</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.643100</td>\n",
       "      <td>0.840000</td>\n",
       "      <td>0.461300</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.474200</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.515600</td>\n",
       "      <td>0.780000</td>\n",
       "      <td>0.437100</td>\n",
       "      <td>0.780000</td>\n",
       "      <td>0.410100</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.599400</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.428300</td>\n",
       "      <td>0.740000</td>\n",
       "      <td>0.750500</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.273800</td>\n",
       "      <td>0.633300</td>\n",
       "      <td>0.185100</td>\n",
       "      <td>0.633300</td>\n",
       "      <td>0.200400</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.403100</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.728300</td>\n",
       "      <td>0.860000</td>\n",
       "      <td>0.697900</td>\n",
       "      <td>0.780000</td>\n",
       "      <td>0.465500</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>0.094900</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>0.546900</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.486800</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>0.434700</td>\n",
       "      <td>0.620000</td>\n",
       "      <td>0.557700</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.676100</td>\n",
       "      <td>0.820000</td>\n",
       "      <td>0.703100</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.615500</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>0.712700</td>\n",
       "      <td>0.775000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE! Installing ujson may make loading annotations faster.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py:2243\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2241\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2242\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2243\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2244\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2248\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py:2559\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2553\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m   2554\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs, num_items_in_batch)\n\u001b[1;32m   2556\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2557\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2558\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m-> 2559\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2560\u001b[0m ):\n\u001b[1;32m   2561\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2562\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   2563\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
